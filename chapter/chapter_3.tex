\chapter{Probabilistic Language Model}
\section{Introduction}
Language model is used to estimate the probability of the word sequences and to estimate the probability of a word following the sequence of words. The concept can be used to auto-complete a sentence with most likely suggestions as shown below:
\begin{figure}[h]
	\centering
	\includegraphics[scale = 0.35]{autoComplete.png}
	\caption{Language Model for Auto-complete}
	\label{fig:my_label}
\end{figure}

\noindent
Some other applications of the language model are:
\begin{enumerate}
	\item Speech Recognition
	\item Spelling Correction
	\item Augmentative Communication
\end{enumerate}

\section{N-gram}
A N-gram is a sequence of N-words. For example:

\textbf{Corpus :} I am happy because I am learning.

\textbf{Uni-grams :} \{I, am, happy, because, I, am, learning \}

\textbf{Bi-grams :} \{I am, am happy, happy because, …\}

\textbf{Tri-grams :} \{I am happy, am happy because, …\}

\section{Sequence Notation}
Let's define a standard sequence notation to represent the sequence of words in our corpus.	Let our corpus be as shown below where n$^{th}$ is represented as W$_n$.
Let the number of words in the corpus be m = 500

\textbf{Corpus:} This is great. ... teacher drinks tea.

Hence,  W$_1$ = This, W$_2$ = is, W$_3$ = great and so on...

Let us represent the sequence of words as W$_s^e$ where s represent the starting index and e represent the ending index of the words in the corpus. So,

W$_1^m$ = W$_1$ W$_2$ W$_3$ ....................  W$_{m-1}$ W$_m$
\\
Similarly,  W$_1^3$ = W$_1$ W$_2$ W$_3$ = This is great

\section{Uni-gram Probability}
Uni-gram Probability is the probability of occurrence of a word in a given corpus.
\\
\textbf{Corpus :} I am happy because I am learning.
\\
\textbf{Size of corpus : } m = 7
\begin{equation}
	P(I) = \frac{2}{7}
\end{equation}
\begin{equation}
	P(happy) = \frac{1}{7}
\end{equation}

Hence, we can see that Probability of uni-gram is given by:
\begin{equation}
	P(w) = \frac{C(w)}{m}
\end{equation}
where C(w) = frequency of the word in the corpus

\section{Bi-gram Probability}
Bi-gram Probability is the probability of occurrence of sequence of two words in a corpus.
\\
\textbf{Corpus :} I am happy because I am learning.
\\
\textbf{Size of corpus : } m = 7
\begin{equation}
	P(am | I) = \frac{C(I\, am)}{C(I)} = \frac{2}{2} = 1
\end{equation}
\begin{equation}
	P(happy | I) = \frac{C(I \, happy)}{C(I)}= \frac{0}{7} = 0
\end{equation}
\begin{equation}
	P(learning | am) = \frac{C(am \, learning)}{C(am)} = 1 / 2
\end{equation}

Hence, we can see that Probability of bi-gram is given by:
\begin{equation}
	P(y | x) = \frac{C(x\, y)}{C(x)}
\end{equation}
where C(w) = frequency of the word in the corpus

\section{Tri-gram Probability}
Tri-gram Probability is the probability of the occurrence of the sequence of three words in a corpus.
\\
\textbf{Corpus :} I am happy because I am learning.
\\
\textbf{Size of corpus : } m = 7
\begin{equation}
	P(happy | I\, am) = \frac{C(I\, am \, happy)}{C(I \, am)} = \frac{1}{2}
\end{equation}

Hence, we can see that Probability of bi-gram is given by:
\begin{equation}
	P(W_3 | W_1^2) = \frac{C(W_1^2\, W_3)}{C(W_1^2)} = \frac{C(W_1^3)}{C(W_1^2)}
\end{equation}
where C(w) = frequency of the word in the corpus

\section{N-gram Probability}
N-gram probability is the probability of the occurrence of the sequence of N-words in a corpus. From uni-gram, bi-gram  and tri-gram probability, we can deduce for N-gram probability denoted by $P(W_N | W_1^{N-1})$ as

\begin{equation}
	P(W_N | W_1^{N-1}) = \frac{C(W_1^{N - 1} \, W_N)}{C(W_1^{N - 1})} = \frac{C(W_1^N)}{C(W_1^{N - 1})}
\end{equation}

\section{Probability of a sequence}
From the conditional probability and chain rule, we have
\begin{equation}\label{CP}
	P(B|A) = \frac{P(A, B)}{P(A)}
\end{equation}

From (\ref{CP}), we get
\begin{equation}\label{CP1}
	P(A, B) = P(A) . P(B | A)
\end{equation}

Using (\ref{CP1}), we can deduce
\begin{equation}
	P(A, B, C, D) = P(A).P(B|A).P(C|A, B).P(D|A, B, C)
\end{equation}

Now, the probability of the sequence of words: "the teacher drinks tea" is given by:

\begin{flalign}
	\begin{aligned}
		 & P(the\, teacher\, drinks\, tea) = P(the) . P(teacher | the) \\
		 & P(drinks | the\, teacher) . P(tea | the\, teacher\, drinks)
	\end{aligned}
\end{flalign}

\section{Approximation of Sequence Probability}
When using n-gram language model, the exact sentence for the required n-gram might not be present exactly in corpus due to which the frequency of the words sequence becomes zero and hence the probability. For example:

\textbf{Input :} the teacher drinks tea

P(the\, teacher\, drinks\, tea) = P(the) . P(teacher \(|\)  the)
P(drinks \(|\) the\, teacher) . P(tea \(|\) the\, teacher\, drinks)

Here,
\begin{equation}\label{snc1}
	P(tea | the\, teacher\, drinks) = \frac{C(the\, teacher\, drinks \, tea)}{C(the\, teacher\, drinks)}
\end{equation}

In the above equation (\ref{snc1}), the frequency of "the teacher drinks tea" and "the teacher drinks" are both likely 0.

Hence, in order to calculate the required probability of the sequence of the words, as
per Markov assumption, only last N words matter. Suppose, only last one words
matter then, we have

P(the\, teacher\, drinks\, tea) \\
= P(the) . P(teacher \(|\) the) P(drinks \(|\) the\, teacher) . P(tea \(|\) the\, teacher\, drinks) \\
$\approx$ P(the) . P(teacher \(|\) the) P(drinks \(|\) teacher) . P(tea \(|\) drinks)

Hence, we can model entire sentence with n-gram following Markov Assumption as follow
\begin{equation}
	P(W_n | W_1^{n - 1}) \approx P(W_n | W_{n - N + 1}^{n - 1})
\end{equation}

\section{Starting and Ending Sentences}
\subsection{Start of sentence symbol $\langle s \rangle$}
For an input sentence, "the teacher drinks tea", we have

P(the\, teacher\, drinks\, tea) $\approx$ P(the) . P(teacher \(|\) the) P(drinks \(|\) teacher) . P(tea \(|\) drinks)

After, we add a start token $\langle s \rangle$, we have

\textbf{Input : } $\langle s \rangle$ the teacher drinks tea

P(the\, teacher\, drinks\, tea) $\approx$ P(the \(|\) $\langle s \rangle$) . P(teacher \(|\) the) P(drinks \(|\) teacher) . P(tea \(|\) drinks)

In this way, we can add start token to determine the possible starting words and calculate the probability of sequence considering first word in N-gram language model. For Tri-gram and N-gram, we add (N - 1) start token $\langle s \rangle$ in the beginning of the sequence.

\subsection{End of sentence symbol $\langle /s \rangle$}
For an input sentence, "the teacher drinks tea", we have

P(the\, teacher\, drinks\, tea) $\approx$ P(the) . P(teacher \(|\) the) P(drinks \(|\) teacher) . P(tea \(|\) drinks)

After, we add a start token $\langle s \rangle$ and an end token $\langle /s \rangle$, we have

\textbf{Input : } $\langle s \rangle$ the teacher drinks tea $\langle /s \rangle$

P(the\, teacher\, drinks\, tea) $\approx$ P(the \(|\) $\langle s \rangle$) . P(teacher \(|\) the) P(drinks \(|\) teacher) . P(tea \(|\) drinks) . P ($\langle /s \rangle$ \(|\) tea)

For bi-gram, tri-gram or any N-gram model, we add end token just once at the end of sentence unlike start token


\section{Count Matrix}
Count matrix is the matrix containing the frequency of occurrence of N-gram. Rows in count matrix represent the unique corpus of (N - 1) gram and columns represent the unique corpus word.

\textbf{Corpus : } $\langle s \rangle$ I study I learn $\langle /s \rangle$

Bi-gram Count Matrix is as follow:

\begin{table}[h]
	\centering
	\begin{tabular}{|c| c| c| c| c| c|}
		\hline
		                     & $\langle s \rangle$ & $\langle /s \rangle$ & I & study & learn \\
		\hline
		$\langle s \rangle$  & 0                   & 0                    & 1 & 0     & 0     \\
		\hline
		$\langle /s \rangle$ & 0                   & 0                    & 0 & 0     & 0     \\
		\hline
		I                    & 0                   & 0                    & 0 & 1     & 1     \\
		\hline
		study                & 0                   & 0                    & 1 & 0     & 0     \\
		\hline
		learn                & 0                   & 1                    & 0 & 0     & 0     \\
		\hline
	\end{tabular}
	\caption{Count Matrix for bi-gram model}
	\label{tab:cmbi2}
\end{table}

\section{Probability Matrix}
Probability matrix is the matrix containing the probability of the occurrence of N-gram words sequence. It can be created by dividing each elements of the count matrix by total sum of all row elements. For example:

\textbf{Corpus : } $\langle s \rangle$ I study I learn $\langle /s \rangle$

\begin{table}[H]
	\centering
	\begin{tabular}{|c| c| c| c| c| c| c|}
		\hline
		                     & $\langle s \rangle$ & $\langle /s \rangle$ & I & study & learn & sum \\
		\hline
		$\langle s \rangle$  & 0                   & 0                    & 1 & 0     & 0     & 1   \\
		\hline
		$\langle /s \rangle$ & 0                   & 0                    & 0 & 0     & 0     & 0   \\
		\hline
		I                    & 0                   & 0                    & 0 & 1     & 1     & 2   \\
		\hline
		study                & 0                   & 0                    & 1 & 0     & 0     & 1   \\
		\hline
		learn                & 0                   & 1                    & 0 & 0     & 0     & 1   \\
		\hline
	\end{tabular}
	\caption{Count Matrix for bi-gram model with row-sum}
	\label{tab:cmbi3}
\end{table}
\quad
\begin{table}[H]
	\centering
	\begin{tabular}{|c| c| c| c| c| c|}
		\hline
		                     & $\langle s \rangle$ & $\langle /s \rangle$ & I & study & learn \\
		\hline
		$\langle s \rangle$  & 0                   & 0                    & 1 & 0     & 0     \\
		\hline
		$\langle /s \rangle$ & 0                   & 0                    & 0 & 0     & 0     \\
		\hline
		I                    & 0                   & 0                    & 0 & 0.5   & 0.5   \\
		\hline
		study                & 0                   & 0                    & 1 & 0     & 0     \\
		\hline
		learn                & 0                   & 1                    & 0 & 0     & 0     \\
		\hline
	\end{tabular}
	\caption{Probability Matrix for bi-gram model}
	\label{tab:cmbi1}
\end{table}

From the above probability matrix, we can calculate the probability of sentence and also predict the next word based on higher probability.

For example :

\textbf{Input} : $\langle s \rangle$ I learn $\langle /s \rangle$

P(Input) = P($\langle s \rangle$ I learn $\langle /s \rangle$)

= P(I \(|\) $\langle s \rangle$) .  P(learn \(|\) I). P ($\langle /s \rangle$ \(|\) learn)

= 1 $\times$ 0.5 $\times$ 1

= 0.5

\section{Generative Language Model}
Generative language model is used to generate a most likely sentences.

Algorithm for the generative language model is:

Step 1 : Choose sentence start

Step 2 : Choose next bi-gram starting with the previous word

Step 3 : Continue until $\langle /s \rangle$ is picked

For example:

Assume our corpus be:

$\langle s \rangle$ Lyn drinks chocolate $\langle /s \rangle$

$\langle s \rangle$ John drinks tea $\langle /s \rangle$

$\langle s \rangle$ Lyn eats chocolate $\langle /s \rangle$

Then, the generative language model can generate a sentence as follow:

Step 1: \colorbox{green!30}{($\langle s \rangle$, Lyn)} or ($\langle s \rangle$, John)

Step 2: (Lyn, eats) or \colorbox{green!30}{(Lyn drinks)}

Step 3: \colorbox{green!30}{(drinks, tea)} or (drinks, chocolate)

Step 4: \colorbox{green!30}{(tea, $\langle /s \rangle$)}

\section{Train, Validation and Test Split}
The recommended split of corpus into train, validation and test data is as follow

\begin{table}[h]
	\centering
	\begin{tabular}{|c |c |c |c |}
		\hline
		Type of corpus & Train  & Validation & Test   \\
		\hline
		Small          & 80$\%$ & 10$\%$     & 10$\%$ \\
		\hline
		Large          & 98$\%$ & 1$\%$      & 1$\%$  \\
		\hline
	\end{tabular}
	\caption{Train, Validation and Test Split}
	\label{tab:TVT Split}
\end{table}

\section{Perplexity}





