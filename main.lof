\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Direct Architecture of Probabilistic Language Model}}{6}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces The Transformer - model architecture}}{7}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Scaled Dot-Product Attention}}{8}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Multi-Head Attention consists of attention layer running in paralle}}{9}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Word2Vec}}{13}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces CBOW Model}}{15}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Glove Model}}{16}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Language Model for Auto-complete}}{18}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Recurrent Neural Networks}}{28}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces An unrolled recurrent neural networks}}{28}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Short Sequence in RNNs}}{29}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Long Sequence in RNNs}}{29}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces The repeating module in a standard RNN contains a single layer.}}{30}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces The repeating module in an LSTM contains four interacting layers.}}{31}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces LSTM Cell}}{31}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Bidirectional RNN}}{34}%
