\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Direct Architecture of Probabilistic Language Model}}{6}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Word2Vec}}{7}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces CBOW Model}}{9}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Glove Model}}{10}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Language Model for Auto-complete}}{12}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Recurrent Neural Networks}}{22}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces An unrolled recurrent neural networks}}{22}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Short Sequence in RNNs}}{23}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Long Sequence in RNNs}}{23}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces The repeating module in a standard RNN contains a single layer.}}{24}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces The repeating module in an LSTM contains four interacting layers.}}{25}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces LSTM Cell}}{25}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Bidirectional RNN}}{28}%
