\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Direct Architecture of Probabilistic Language Model}}{6}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces The Transformer - model architecture}}{7}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Scaled Dot-Product Attention}}{8}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Multi-Head Attention consists of attention layer running in paralle}}{9}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Word2Vec}}{11}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces CBOW Model}}{13}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Glove Model}}{14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Language Model for Auto-complete}}{16}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Recurrent Neural Networks}}{26}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces An unrolled recurrent neural networks}}{26}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Short Sequence in RNNs}}{27}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Long Sequence in RNNs}}{27}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces The repeating module in a standard RNN contains a single layer.}}{28}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces The repeating module in an LSTM contains four interacting layers.}}{29}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces LSTM Cell}}{29}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Bidirectional RNN}}{32}%
